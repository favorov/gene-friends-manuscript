\documentclass{llncs}
%\usepackage{fancyhdr}
\pagestyle{plain}
%\pagestyle{headings}
\usepackage{standalone}
\usepackage{graphicx} 
\usepackage{comment}
%\usepackage{xcolor}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{cite}
%\usepackage{natbib}
%\usepackage[title]{appendix}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{latexcolors}
\usepackage{import}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary {arrows.meta}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}

%


%\title{Telling the story of best friends}
%
\title{Friends test: a self-tuning approach for detecting specific strong associations in bipartite graphs}
%
\titlerunning{Friends test} 
% abbreviated title (for running head)
% also used for the TOC unless
% \toctitle is used
%
\author{
Alexandra Suvorikova \inst{1} \and Alexei Kroshnin \inst{1} \and Dmirijs Lvovs\inst{6} \and Vasily Ramensky \inst{2,3,4} \and Vera Mukhina \inst{5,6} \and Ludmila Danilova\inst{6} \and Andrey Mironov \inst{4,7} \and Alexander Favorov\inst{6,8}}
%
\authorrunning{A. Suvorikova et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Alexandra Suvorikova, Alexey Kroshnin, Dmitrijs Lvovs, Vasily Ramensky, Vera Mukhina, Ludmila Danilova, Andrey Mironov, and Alexander Favorov}
%
\institute{
Weierstrass Institute, Berlin, Germany, 
\email{suvorikova@wias-berlin.de}
\and
MSU Institute for Artificial Intelligence, \\Lomonosov Moscow State University, Moscow, 119992, RF
\and
National Medical Research Center for Therapy and Preventive Medicine of the Ministry of Healthcare of Russian Federation, Moscow, 101990, RF
\and
Department of Bioengineering and Bioinformatics, \\Lomonosov Moscow State University, Moscow, 119992, RF
\and
University of Maryland School of Medicine, Baltimore, MD 21205, USA
\and
Johns Hopkins University School of Medicine, \\ Baltimore, MD 21205, USA, \email{favorov@sensi.org}
\and
The Institute for Information Transmission Problems, Moscow, 127051, RF
\and
Vavilov Institute of General Genetics, RAS, Moscow, 119333, RF
}

\maketitle % typeset the title of the contribution

\renewcommand{\tag}{tag}
\newcommand{\node}{node}
\newcommand{\T}{T}
\newcommand{\C}{C}
\newcommand{\tl}{t}
\newcommand{\cl}{c}
\newcommand{\test}[1]{\textbf{\textit{#1}}}

\begin{abstract}
We propose a novel approach to select hidden specific strong connections in a dataset represented by a bipartite graph. \textcolor{red}{...} This model fits many practical problems, such as gene expression regulation by a set of transcription factors, etc. The method is available as an \textsf{R} package at \url{https://github.com/favorov/best.friends}.
\end{abstract}


\keywords{weighted bipartite graph, rank statistics, feature selection, clustering, knowledge transfer, specific gene regulation, pattern marker}

\section{Introduction}

A bipartite graph $G = (V, E)$ is a network structure whose set of vertices $V$ is partitioned into two disjoint subsets $T$ and $C$, i.e., $
T \cup C = V$,  $T \cap C = \varnothing$. In such graphs, edges connect only vertices in $T$ to vertices in $C$; no edges exist between vertices within the same subset. Fig. \ref{fig:nice_name} provides an example. 
\begin{figure}
 \centering
 \import{}{bipartite}
 \caption{\red{draw a new graph}}
 \label{fig:nice_name}
\end{figure}

In many applications, $T$ and $C$ represent entities of different types, and the edges are weighted. 
The corresponding weighted adjacency matrix $\mathcal{A}_G$ takes a block form,
\begin{equation}
\label{eq:adj_matrix}
\mathcal{A}_G := \begin{pmatrix}
0 & A\\
A^{T} & 0
\end{pmatrix},
\quad \text{where}~~
A := \begin{pmatrix}
a_{11} & a_{12} & \dots & a_{1k} \\
 &\cdots & \cdots & \\
a_{n1} & a_{n2} & \dots & a_{nk}
\end{pmatrix},
\quad k := |C|, \quad
n := |T|,
\end{equation}
where $|T|$ and $|C|$ are the numbers of vertices in $T$ and $C$, respectively. 
In the case of unweighted graph, $a_{ij} = 1$ if there is an edge between vertices $t_i\in T$ and $c_{j}\in C$, $a_{ij} = 0$ otherwise. However, in many cases $a_{ij}$ is a real number reflecting the association strength between vertices---for example, the distance between vertices, probability that the connection exists, correlation, etc. 

The bipartite graph naturally emerges in various real-world settings, including recommender systems \textcolor{red}{[...]}, 
resource allocation problems \textcolor{red}{[...]}, etc. This model effectively captures intrinsic relationships in complex data structures, such as decomposed omic data \cite{fertig_cogaps_2010, stein-obrien_enter_2018}, \textcolor{red}{phenotype-disease} matrices, and similar datasets. In such problems, the vertices in one part (say, $T$) correspond to features (e.g., genes, phenotypes, etc.), while the vertices in the other part (say, $C$) correspond to the biological processes or conditions (e.g., \textcolor{red}{...}) in which the features participate. The edge weights indicate the association strength: the larger the weight, the stronger the relationship between the corresponding feature and the process.

In this study, we generalize the setting introduced in \cite{stein-obrien_patternmarkers_2017}. Our focus is on identifying those vertices in $T$ that exhibit a \textit{specific pattern of strong associations}---that is, vertices that are strongly connected to only a limited number of vertices in
$C$ while having relatively weaker associations with the others. \textcolor{red}{This characteristic is essential, for example, in applications where one seeks to pinpoint features that are selectively related to certain biological processes or conditions.}\red{[...]}

Before presenting our approach, we discuss briefly some related problems.

\paragraph{Assignment problem.} In many applications, the aim is to find the pairs of closest vertices $t\in T$ and $c \in C$ \red{[...]}. 
This problem can be naturally stated as the assignment problem \red{[...]}. Let a distance function be $d \colon T \times C \rightarrow \mathbb{R}_{+} $. The goal is to find an injection $f: T \rightarrow C$ such that the total distance is minimum, i.e., $\sum_{t\in T}D(t, f(t)) \rightarrow \min_{f}$.

\paragraph{Detection of hubs}
Traditionally, hubs are defined as vertices with an unusually high degree. In many settings, they are marked as influential nodes \cite{shiokawa2015scan}. By visualizing and analyzing hubs, one can gain insights into how networks represented by graphs are structured \cite{royer2022epilepsy}. In contrast, other studies treat hubs as outliers \cite{kirkley2024identifying}. Informally, the hub identification problem aims to highlight vertices with abnormally high connectivity.

\paragraph{Search for specific connections.} In certain applications, the objective is to identify the pairs of vertices satisfying specific properties. Stein et al. \cite{stein-obrien_patternmarkers_2017} examine a set of features $T$ and a set of biological processes $C$, to pinpoint markers: features that are strongly associated with exactly one process. 
\vspace{12pt}

Our goal is to identify all vertices in $T$ that are strongly associated with a limited subset of vertices in $C$. This formulation extends the setting of \cite{stein-obrien_patternmarkers_2017}, where the goal is to identify all vertices in $T$ that are strongly associated with exactly one vertex in $C$. 

In our work, the association strengths are the edge weights stored in the block $A$ of the adjacency matrix \eqref{eq:adj_matrix}. Let us now introduce the edge weight-generating model.
Under the null hypothesis, we assume that all edge weights originate from a common distribution, but have been modified through unknown transformations. Let $\xi_{ij}, 1 \le i \le n,\; 1 \le j \le k$ be i.i.d.\ latent random variables drawn from unknown distribution. We assume that there exists a fixed collection of unknown monotone functions $f_1, \dots, f_k$ on $\mathbb{R}$
such that for each edge $(i,j)$ the corresponding edge weight is $a_{ij} = f_j(\xi_{ij})$.
Moreover, each function 
$f_j$ depends solely on the vertex $c_j$, meaning that the form of the transformation is determined entirely by the corresponding column vertex. Without loss of generality, we restrict our attention to the case where 
$a_{ij}\ge 0$ for all $(i,j)$ and interpret $a_{ij}$
as the distance between $t_i$ and $c_j$.

Introducing transformations $f_1, \dots, f_k$, we let the vertices in $C$ (e.g., the observed biological processes) be of different natures. As a consequence, the distance between a fixed vertex $t\in T$ and arbitrary vertices $c, c' \in C$ cannot be directly compared on an absolute scale. Therefore, the concepts of ``close'' and ``distant'' vertices are inherently relative and must be interpreted taking into account all elements in $A$. In other words, the closeness criteria depend on the structure of the entire matrix $A$, precluding the use of a universal threshold. To address this challenge, we propose \textit{friends.test}--a novel self-tuning approach that identifies vertices $t\in T$ having a limited number of ``close'' (or \textit{friendly}) vertices in $C$ and also determines precisely which vertices in $C$ constitute these ``friends''. The \textit{friends.test} consists of two steps: \textcolor{red}{ranking and model fitting.}

\textbf{Step 1: ranking.} First, we use the ranking procedure to  \textcolor{red}{remove} heterogeneity induced by unknown monotone transformations $f_1, \dots, f_k$. The ranking is performed independently for each vertex $c_j \in C$. Specifically, we replace elements $a_{ij} \in A$ with their ranks computed with respect to other values stored in the corresponding ($j$-th) column. If the rank of $a_{ij} \in A$ and the rank of $a_{il}\in A$ coincide, we say that the vertices $c_l$ and $c_j$ are equidistant from the vertex $t_i$. Conversely, if the rank of $a_{ij}$ is lower than the rank of $a_{il}$, we say that the vertex $c_j$ is closer to the vertex $t_i$ as the vertex $c_l$.


\textbf{Step 2: model fit.} After ranking we filter out so-called \textit{\textcolor{red}{global} hubs}. We say that $t_i $ is a global hub if (1) it has connections to almost all vertices in $C$; (2) the corresponding edge weights $a_{i1}, \dots, a_{ik}$ are statistically identical (i.i.d.). Section~\ref{sec:method} presents a statistical criterion identifying global hubs. Informally, hub filtering removes the vertices in 
$T$ that do not differentiate meaningfully between distinct subsets of the nodes in $C$. Their connectivity pattern suggests that the corresponding connections are formed by chance, rather than through any structured process.  
For the remaining $t_i\in T$, we fit a self-tuning parametric model designed to identify their ``close'' vertices from $C$. Section~\ref{sec:discussion} discusses a particular choice of the parametric model.

The model fit is performed separately for each vertex $t\in T$. Since there is typically no a priori preference for a particular vertex $t$, one can fit the model for all relevant vertices simultaneously. In this case, one should apply the multiplicity correction. 

\paragraph{Computational aspects.} Denoting the cardinality of $T$ as $n$, and the cardinality of $C$ as $k$ ($|T| = n$, $|C| = k$), one can estimate the computational complexity of \textit{friends.test} as $\mathcal{O}(kn\log(n))$.
\textcolor{red}{The approach suits well for the large data sets.}

\paragraph{R-package}
The software implementing the \text{friends test} is available as an \textsf{R} package \textit{friends.test} at 
\url{https://github.com/favorov/best.friends}. The package vignette shows simple use cases.


\paragraph{The organization of the paper} Section~\ref{sec:method} introduces the \textit{friends.test}. Section~\ref{sec:experiments} presents the experiments. Section~\ref{sec:discussion} discuss...

\section{\textit{Friends}-test}
\label{sec:method}
Let $C := \{c_1, \dots, c_k\}$, and let $T := \{t_1, \dots, t_n\}$. 
Without loss of generality, we assume that the elements of $A$~\eqref{eq:adj_matrix} are distances.  
 We will denote the $i$-th row of the matrix $A$ as $\text{row}_i(A)$ and its $j$-th column $\text{col}_j(A)$, i.e.,
\[
\text{row}_i(A) := (a_{i1}, \dots, a_{ik}),
\quad
\text{col}_j(A) := (a_{1j}, \dots, a_{nj}).
\]
In other words, a row $\text{row}_i(A)$ corresponds to the distances between the vertex $t_i$ to all vertices in $C$. A column $\text{col}_j(A)$ corresponds to the distances from a vertex $c_j$ to all vertices in $T$. 

% In the remainder of the text, we assume that all $a_{ij}$ are independent random variables.

% \textcolor{red}{Further, under $H_0$} we assume that the attention that each node $c_j$ from $C$ pays to all tags in $T$ is identically distributed. In other words, values in $\text{col}_j(A)$ have the same distribution. We denote it as $P_j$. Typically, all distributions $P_1, \dots, P_k$ are unknown and may vary in nature. 


For each node $c_j \in C$, we rank the elements in the corresponding column $\text{col}_j(A)$ in decreasing order---that is, higher $a_{ij}$ receive lower ordinal numbers. Thus, each entry $a_{ij} $ in $\text{col}_j(A)$ is assigned the ordinal number---rank $r_{ij}$. In cases where multiple entries in $\text{col}_j(A)$ share the same value, we use a randomized tie-breaking procedure.

We denote as $R$ the matrix containing the ranks $r_{ij}$, 
\begin{equation*}
R = \begin{pmatrix}
r_{11} & r_{12} & \dots & r_{1k} \\
 &\cdots & \cdots & \\
r_{n1} & r_{n2} & \dots & r_{nk}
\end{pmatrix}, 
\quad
r_{ij} :=\text{rank}\left(a_{ij}~ \text{inside}~\text{col}_j(A)\right).
\end{equation*}
Let the $i$-th row of $R$ as $\text{row}_i(R) := (r_{i1}, \dots, r_{ik})$. To identify if a vertex $t_i\in T$ is connected to only a limited set of vertices in $C$, we fit a parametric model to observations stored in $\text{row}_i(R)$. However, first, we have to filter out the global hubs. We say that $t_i\in T$ is a global hub, \textcolor{red}{if $H_0$} holds true---that is, for all $j = 1, \dots, k$,
$r_{ij} \sim \text{Unif}(\{u_i, \dots, w_i\})$ and $1\le u_i \le w_i \le n$. \textcolor{red}{To verify whether $H_0$ holds}, one can either use a test assessing the uniformity of the sample $\text{row}_i(R)$, or apply an information criterion, which will be introduced later in the text (see paragraph \textit{Information Criterion}) 

Next, we assume that vertex $t_i$ is not a global hub. Since we now consider only a single vertex, we omit the index $i$ for simplicity. Specifically,
\[
\text{row}_{i}(R) = (r_{1}, \dots, r_{k}).
\]
\textcolor{red}{Under $H_1$}, we assume that each $r_l$ ($1\le l \le k$) comes from a mixture of two uniform distributions on the discrete grid $\{u, \dots, w\}$, where $1\le u < v \le n$, i.e,
\[
r_l \sim p^* \text{Unif}\left(\{u, \dots, m^*\}\right) + (1-p^*)\text{Unif}\left(\{m^{*}+1, \dots, w\}\right),
\]
where $p^* \in (0, 1)$ and $m^*$ are unknown parameters.
Consequently, the probability to observe $r_l \in \{u, \dots, m^*\}$ is $\frac{p^{*}}{m^* - u + 1}$, and the probability $r_l \in \{ m^*+1, \dots, w\}$ is $\frac{1 - p^{*}}{w - m^*}$.
To estimate $p^*$ and $m^*$ from data, we use Maximum Likelihood. Specifically, the corresponding  \textcolor{red}{log-likelihood} of the mixture is written as
\[
L(p, m; r_1, \dots, r_k) := s\ln\left(\frac{p}{m-u+1}\right) + (k-s)\ln\left(\frac{1-p}{w - m}\right),
\]
where $ s$ is such that $u \le r_{s} \le m$ and $m < r_{s+1} \le w$. Optimizing $L(p, m; r_1, \dots, r_k)$ over $p$ yields $\hat{p} = \frac{s}{k}$. This ensures that one can use brute-force search over $m$ to find the optimal parameters.

\paragraph{Information criterion}
The homogeneity test can be avoided by employing the following information criterion, which relies on preliminary knowledge about the data. Specifically, one needs to assume the likelihood that the vertex $t$ has ``close'' vertices in $C$. We denote this probability by $q \in (0, 1)$ and set
\[
L_1 := L(\hat{p}, \hat{m}) + \ln(q),
\quad
L_2 := L(0, 0) + \ln(1-q).
\]
The best fit corresponds to $\max\{L_1, L_2\}$.



% \subsection{Symmetric attention matrix $A$}
% % \textcolor{blue}{\textit{The matrix shows relation tags vs node and it is not quadratic. How can the non-quadratic matrix be symmetric?}}
% It may happen that the number of tags $n$ coincides with the number of nodes $k$, $n = k$. The particular case of the symmetric (by construction) attention matrix $A$ requires a more detailed investigation.

% The $p$-value calculation relies on the assumption of the independence of attention values in different nodes. If the attention matrix $A$ is symmetric by the nature of the underlying bipartite graph, this assumption \textcolor{red}{does not hold}. Thus, the theoretical inference presented in Section~\ref{sec:theory} is not valid anymore.

% Still, the numerical procedure works. We show this in more detail in Supplement~\ref{seq:symmetric_a}. Of note, sometimes we know that all the diagonal elements are $0$ by construction. In this case, both tests use 
% \[
% p(w) = (1-w)^{k-1}, ~~\text{cf. equation \eqref{eq:pw}.}
% \]


\subsection{Multiple testing}
\label{sec:multimurkers}

All the tests we formulated here are not corrected for the multiplicity of hypotheses. Namely, they work directly if we \textit{a-priori} know what tag $t_i$ and what size $l$ of friends set we run the test for. 

In practice, the tests are run for each tag or even for each tag and the friend set population. Two important observations follow.

\paragraph*{Multiplicity correction} 
To run the \test{best friend test} on all the $n$ tags, we 
calculate $n$ $p$-values. To run the \test{friends test}, we calculate $n \cdot k$ $p$-values. However, all the tests rely on the ranking of the elements inside the same attention matrix $A$. Thus, the assumption of test independence does not hold. In this case, the standard Bonferroni correction on the set of corresponding $p$-values is possibly too strong (\cite{cabin2000bonferroni}). However, some correction is still necessary. We leave it to the scope of the particular application.

\paragraph*{Multimarkers} Note that after the multiple hypothesis correction, a node may be the best friend (or an element of the true friends set) for more than one tag. The set of tags is thus a multimarker for the node. In practice, a multimarker tags (selects) a node more specific than each of its elements.

\subsection{Friends and antagonists}





\section{Application examples}
\label{sec:experiments}
\subsection{\red{Senteiment analysis}}
\paragraph{Data} To illustrate the performance of the 
\textbf{friends} test, we use AffectVec data \red{[cite]}. This is a word emotion database capturing the subtlety of the English language by providing over 70,000 words annotated with intensity scores for more than 200 emotions. AffectVec quantifies the degree to which each word evokes a wide range of emotional responses. For example, the word ``prank'' may primarily convey joy. Yet it can also be associated with fear, suspense, or a blend of other emotions.

AffectVec is organized as a tabular. Each row corresponds to an individual word and each column represents one of the more than 200 emotion categories. In other words, words are \textit{tags}, emotion categories are \textit{nodes} and the corresponding intensity scores are \textit{attentions}. 

\paragraph{Data preprocessing} To test the \textbf{friends} test, we selected $1080$ adjectives (\red{using Python}). \red{Appendix} presents the selection procedure. The data is available at \red{url}.

\red{We consider two settings}: friends, anti-friends. Full list of emotions and manually filtered list of emotions.

Multiplicity correction: $\frac{1}{1080}$

\subsection{Experiment by AF...}
\section{Discussion}
\label{sec:discussion}

The method works even if the modeling assumption is misspecified...

\textcolor{blue}{Sensitivity to outliers!!!}
\textcolor{red}{We note that the step function can be replaced by some other shapes... Why step? Clustering? Bump?}
\textcolor{green}{Works under model misspecification}

In this manuscript, we develop a method and software to detect noteworthy edges in a weighted bipartite graph. We suppose all edges of the graph to be co-directed. The graph models a directed relation (referred to as attention) from the vertices of one part (nodes) to the vertices of another part (tags).

Essentially, the method consists of two steps. First, we use a double-ranking approach to find the putative friends. Then, to validate the friendship hypothesis, we perform a novel statistical test that is distribution-free.

Along with the single node procedure (\test{best friend test}), we suggest its extension for a subset of nodes (\test{friends test}).

The \text{best friend test} is a particular case of the \test{friends test}, but we consider it separately in the software and, hence, in the methods. Namely, \test{friends test} has higher computational costs and it requires multiplicity correction even for one tag (Section \ref{sec:multimurkers}). In many cases, the \text{best friend test} is enough for practical applications. 

Although the problem looks abstract, its solution has numerous straightforward applications. For instance, the detection of the marker genes \cite{stein-obrien_patternmarkers_2017} for expression patterns critically simplifies the biological interpretation of the results of transcription matrix factorization \cite{Stein_2018,Fertig_2016}. Here, the genes are tags and the patterns are nodes. If a pattern is a friend of a gene (see Section \ref{sec:method}), the gene is the marker of the pattern.

However, the theoretical result is limited to the case of an asymmetric attention matrix. If the matrix is symmetric by the design, the null hypothesis does not hold. However, the computational experiment \textcolor{red}{(see Supplement)} shows that the independence proposition \textcolor{red}{can be used}. Thus, the method applies to the analysis of, e.g., distance matrices. 

The first possible area of application is feature selection. By identification of markers, instead of all tags, we can use a relatively small subset for further analysis. Moreover, the identification of friend-marker pairs helps to remove non-specific connections from a graph. 

Second, the proposed method is useful for efficient clustering of a set of selected features. Also, the friendship concept provides a new similarity measure that possibly generates more interpretable clustering, than the clustering with $A$ being a similarity measure.

Another possible direction is knowledge transfer: if we know something new about Augustus, we know something new about his friends. 
\subsection{}


% \textcolor{blue}{+ Friedman test }
% %\url{https://en.wikipedia.org/wiki/Friedman_test}


\section{Conflict of interest}
The authors declare no conflict of interest.

\section{Acknowledgements}
AF acknowledges support by National Institutes of Health (NIH) P30CA006973 and 1U01CA253403-01.
Thanks to Daniel Shu and Caedmon Haas for the translation of the motto to gold Latin. 

\bibliography{gene-best-friends}

\bibliographystyle{splncs03}
%\begin{subappendices}

\newcommand{\beginsupplement}{%
 \setcounter{table}{0}
 \renewcommand{\thetable}{S\arabic{table}}%
 \setcounter{figure}{0}
 \renewcommand{\thefigure}{S\arabic{figure}}
 \setcounter{equation}{0}
 \renewcommand{\theequation}{S\arabic{equation}}%
 }

\newpage
\section*{Supplement}
\beginsupplement
\subsection{Data preprocessing} 
The full list of emotion categories used in the experiment is following:
\textit{joy, surprise, trust, anticipation, fear, anger, sadness, disgust, happiness, levity, hate, loyalty, melancholy, anxiety, embarrassment,  regard, stress, gusto, compunction, cynicism, situation, umbrage, favor, meekness, compassion, withdrawal, scare, unrest, calm, courage, despair, fidget, shyness, apathy, hysteria, shadow, resentment, optimism, heartstrings, bonheur, dudgeon, merriment, hope, foreboding, envy, interest, relaxed, cruelty, helplessness,   solicitude, satisfaction,   suspense, fondness, dolor, weakness, electricity, esteem, woe, relieved, wonder, attachment, pessimism, malice, love, compatibility, timidity, blessedness, exultation, tumult, alienation, humility, powerlessness, complacency, gloom, aggression, sensation, antipathy, gloat, doubt, empathy, consciousness, ingratitude, hopelessness, signal, alarm, dislike, stir, distance, smugness, repentance, easiness, friendliness, gravity, displeasure, discouragement, pique, benevolence, chagrin, tension, togetherness, panic, eagerness, pleasure, excitement, mood, animosity, defeatism, worship, repugnance, grudge, euphoria, antagonism, trait, brotherhood, stewing, pity, daze, sympathy, annoyance, encouragement,  buoyancy, devotion, triumph, contempt, belonging, sinking, unhappiness, trepidation, admiration, disapproval, indifference, affection, astonishment, oppression, languor, coolness, liking, behaviour, peace, misogyny, bang, cheerfulness, creeps, agitation, boredom, gratification  hurt, agape, concern, ardor, mourning, harassment, contentment, closeness, surprised, confusion, presage, approval, state, wrath, dander, reverence, content, amusement, indignation, fearlessness, depreciation, expectation, tenderness, misery, depression, forgiveness, willies, fit, comfort, shame, apprehension   delight, jealousy, aggravation, chill, warpath, serene, exuberance, resignation, gratitude, despondency, nirvana, lividity, emotion, disappointment horror, grief, weight, distress, intoxication, irritation, insecurity, pride, fever, rejoicing, impatience, politeness, tranquillity, hilarity, fury, gladness, thing, nausea, calmness, fulfillment, ecstasy, elation, playfulness, exhilaration, titillation, gratefulness, diffidence, radiance, sorrow, confidence, security, ego, hostility, frustration, attrition, angst, shock, preference, enthusiasm, isolation, conscience, scruple, worry, earnestness, malevolence, awe, guilt, identification.}



The filtered list of emotions
\textit{joy, happiness, surprise, gratitude, friendliness, hope, admiration, love, cheerfulness, trust, interest,
compassion, stress, fear, anger, sadness, hate, melancholy, anxiety, despair, powerlessness, aggression, dislike, cynicism, unrest, apathy, hysteria, calm, courage, shyness, 
 optimism, helplessness, weakness, pessimism, humility, antipathy, 
 togetherness, panic, eagerness, pleasure, excitement, euphoria, brotherhood, sympathy, annoyance, triumph, belonging, unhappiness, admiration, disapproval, indifference, affection, astonishment, 
 agitation, boredom, gratification, concern, harassment, contentment, closeness, confusion, approval, amusement, indignation, fearlessness, depreciation, expectation, 
 tenderness, misery, depression, forgiveness, fit, comfort, shame, apprehension, delight, jealousy, aggravation, chill, disappointment, horror, grief, distress, intoxication,
 irritation, insecurity, pride, impatience, politeness, tranquillity, calmness, playfulness, gratefulness, sorrow, confidence, security, ego, frustration, shock, enthusiasm, isolation, worry, guilt.}






\end{document}


