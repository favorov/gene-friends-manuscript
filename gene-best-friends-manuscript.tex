\documentclass{llncs}
%\usepackage{fancyhdr}
\pagestyle{plain}
%\pagestyle{headings}
\usepackage{standalone}
\usepackage{graphicx} 
\usepackage{comment}
%\usepackage{xcolor}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{cite}
%\usepackage{natbib}
%\usepackage[title]{appendix}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{latexcolors}
\usepackage{import}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary {arrows.meta}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\begin{document}

%


%\title{Telling the story of best friends}
%
\title{Friends test: a self-tuning approach for detecting specific strong associations in bipartite graphs}
%
\titlerunning{Friends test} 
% abbreviated title (for running head)
% also used for the TOC unless
% \toctitle is used
%
\author{
Alexandra Suvorikova \inst{1} \and Alexei Kroshnin \inst{1} \and Dmirijs Lvovs\inst{6} \and Vasily Ramensky \inst{2,3,4} \and Vera Mukhina \inst{5,6} \and Ludmila Danilova\inst{6} \and Andrey Mironov \inst{4,7} \and Alexander Favorov\inst{6,8}}
%
\authorrunning{A. Suvorikova et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Alexandra Suvorikova, Alexey Kroshnin, Dmitrijs Lvovs, Vasily Ramensky, Vera Mukhina, Ludmila Danilova, Andrey Mironov, and Alexander Favorov}
%
\institute{
Weierstrass Institute, Berlin, Germany, 
\email{suvorikova@wias-berlin.de}
\and
MSU Institute for Artificial Intelligence, \\Lomonosov Moscow State University, Moscow, 119992, RF
\and
National Medical Research Center for Therapy and Preventive Medicine of the Ministry of Healthcare of Russian Federation, Moscow, 101990, RF
\and
Department of Bioengineering and Bioinformatics, \\Lomonosov Moscow State University, Moscow, 119992, RF
\and
Institute for Genome Sciences, University of Maryland School of Medicine, Baltimore, MD 21205, USA
\and
Johns Hopkins University School of Medicine, \\ Baltimore, MD 21205, USA, \email{favorov@sensi.org}
\and
The Institute for Information Transmission Problems, Moscow, 127051, RF
\and
Vavilov Institute of General Genetics, RAS, Moscow, 119333, RF
}

\maketitle % typeset the title of the contribution

\renewcommand{\tag}{tag}
\newcommand{\node}{node}
\newcommand{\T}{T}
\newcommand{\C}{C}
\newcommand{\tl}{t}
\newcommand{\cl}{c}
\newcommand{\test}[1]{\textbf{\textit{#1}}}

\begin{abstract}
We propose a novel approach to identify specific connectivity patterns in a dataset represented by a bipartite graph. This model fits many practical problems, such as gene expression regulation by a set of transcription factors, etc. The method is available as an \textsf{R} package at \url{https://github.com/favorov/best.friends}.
\end{abstract}


\keywords{weighted bipartite graph, rank statistics, feature selection, clustering, knowledge transfer, specific gene regulation, pattern marker}

\section{Introduction}
Graphs whose vertices belong to two sets (partitions) arise naturally in many modern applications: recommendation systems, social network analysis, medicine, etc. Particular medical applications include drug-adverse
reaction associations \cite{timilsina2019discovering}, relationships between genes \cite{fertig_cogaps_2010, stein-obrien_enter_2018}, phenotype-driven disease prioritization  \cite{ullah2015estimating}. In such scenarios, the objects in one partition interact exclusively with the objects in another, giving rise to the weighted bipartite graph model. Edge weights typically encode association strength, such as correlation, interaction probability, or distance.

Analyzing such graphs reveals particular connectivity patterns, such as genes participating in only a few biological processes. The work of Stein-O’Brien et al. \cite{stein-obrien_patternmarkers_2017} illustrates this idea by analyzing a gene–process weighted bipartite graph and pinpointing marker genes---genes strongly associated with exactly one biological process. Identifying connectivity patterns is also valuable in the analysis of protein-protein \red{interaction maps}: prioritizing proteins that remain highly connected, but interact strongly with only a narrow set of biological processes can improve therapeutic specificity and reduce off-target effects \cite{viacava2021centrality}.

Building on this motivation, we develop a procedure for identifying the vertices with a specific connectivity pattern: vertices in one partition (say, $T$) that form strong links to only a small subset of vertices in the opposite ($C$) partition. The remainder of this section establishes notation, states the formal problem, and sketches our solution.

A weighted bipartite graph $G = (V, E)$ is a network structure whose set of vertices $V$ is partitioned into two disjoint subsets $T$ and $C$. The edges connect only the vertices in $T$ to the vertices in $C$.
Fig. \ref{fig:nice_name} provides an example. 
\begin{figure}
 \centering
 \import{}{bipartite}
 \caption{Illustration of a weighted bipartite graph where vertices belong to two disjoint sets, $T = \{t_1, \dots, t_n\}$ and $C = \{c_1, \dots, c_k\} $.
Edges represent interactions between elements of different sets, with weights $a_{ij}$ encoding association strength.}
 \label{fig:nice_name}
\end{figure}
The corresponding weighted adjacency matrix $\mathcal{A}_G$ takes a block form,
\begin{equation}
\label{eq:adj_matrix}
\mathcal{A}_G := \begin{pmatrix}
0 & A\\
A^{T} & 0
\end{pmatrix},
\quad \text{where}~~
A := \begin{pmatrix}
a_{11} & a_{12} & \dots & a_{1k} \\
 &\cdots & \cdots & \\
a_{n1} & a_{n2} & \dots & a_{nk}
\end{pmatrix},
\quad k := |C|, \quad
n := |T|,
\end{equation}
where $|T|$ and $|C|$ are the numbers of vertices in $T$ and $C$, respectively. In the case of unweighted graph, $a_{ij} = 1$ if there is an edge between vertices $t_i\in T$ and $c_{j}\in C$ and $a_{ij} = 0$ otherwise. If the graph is weighted, $a_{ij}$ are real numbers reflecting the association strength between vertices. 

Each column in $A$ (i.e., $(a_{1j}, \dots, a_{nj})$) contains edge weights corresponding to the association strengths between a column vertex $c_j$ (e.g., a biological process) and all row vertices in $T$ (e.g., genes). Because every column may follow its own scale or distribution, weights from different columns are generally not comparable on an absolute scale. We model this effect by introducing latent variables. Specifically, \red{under the null hypothesis}, we assume that all edge weights originate from a common distribution, but have been modified through unknown transformations. 

Let $\xi_{ij}, 1 \le i \le n,\; 1 \le j \le k$ be i.i.d.\ latent random variables drawn from unknown distribution $P$. We assume that for each column vertex $c_j$ there exists a fixed unknown monotone functions $f_j$ on $\mathbb{R}$
such that for each edge $(i,j)$ the corresponding edge weight is $a_{ij} = f_j(\xi_{ij})$. The higher values $a_{ij}$ indicate a stronger association between $t_i$ and $c_j$. With this convention, our goal is to identify the vertices in $T$ whose strong associations are concentrated on only a small (depending on $t_i\in T$) subset of vertices in $C$. We refer to this subset as ``friends'' of $t_i$.

Recall that the column-specific monotone distortions $f_1, \dots, f_k$ capture the fact that the entries $a_{ij}$ in different columns are of different nature. Hence, they cannot be directly compared on an absolute scale. Furthermore, whenever a row vertex $t_i$ does have a small group of column vertices in $C$ to which it is strongly linked, that group is typically unique to $t_i$. Together, the lack of a common scale and the vertex-specific friend sets pose two statistical challenges: scale invariance and per-row adaptivity. To address these challenges, we introduce the \textsf{friends.test}, a computationally efficient self-tuning approach composed of two steps: ranking and model fitting.

First, we apply the ranking resolving monotone distortions and mapping all entries onto a common scale. The ranking is performed independently for each vertex $c_j \in C$. Specifically, we replace entries $a_{ij}$ with their ranks computed with respect to other entries stored in the same ($j$-th) column. Let the rank of $a_{ij}$ be $r_{ij}$. 

After ranking, we filter out non-informative vertices $t_i \in T$---such $t_i$ whose ranks $(r_{i1}, \dots, r_{ik})$ follow a discrete uniform law. Indeed, they do not differentiate meaningfully between distinct subsets of vertices in $C$.  This connectivity pattern indicates that the corresponding connections are formed by chance within the $C$ partition.

Row vertex $t_i$ rejected by the uniformity test is referred to as \textit{selective}. Let the corresponding row of ranks be $(r_{i1}, \dots, r_{ik})$ and denote $F_i = \{c_j\in C\, :\, r_{ij} \le m\}$, where $m$ is an unknown true cut-off threshold differentiating between strong associations and all others. In other words, $F_i$ is a set of friends of a selective vertex $t_i$. To identify $F_i$, we fit a self-tuning parametric model. Section~\ref{sec:discussion} discusses a particular choice of the parametric model.

The \textsf{friends.test} runs in $\mathcal{O}(nk\log(n))$ \red{times}, making it scalable for large matrices. The software is available as an \textsf{R} package \textsf{friends.test} at\\ 
\url{https://github.com/favorov/best.friends}. The package vignette shows simple use cases.


%%\paragraph{The organization of the paper} Section~\ref{sec:method} introduces the \textit{friends.test}. Section~\ref{sec:experiments} presents the experiments. Section~\ref{sec:discussion} discuss...

\section{The \textsf{friends.test}}
\label{sec:method}
Let the $i$-th row of the matrix $A$ be $\text{row}_i(A)$ and let the $j$-th column be $\text{col}_j(A)$,
\[
\text{row}_i(A) := (a_{i1}, \dots, a_{ik}),
\quad
\text{col}_j(A) := (a_{1j}, \dots, a_{nj}).
\]
For each column node $c_j \in C$, we rank the entries in $\text{col}_j(A)$ in decreasing order: the higher $a_{ij}$ receives the lower rank. In cases where multiple entries in $\text{col}_j(A)$ share the same value, we use a randomized tie-breaking procedure. Let $R$ be a matrix containing the ranks $r_{ij}$, 
\begin{equation*}
R = \begin{pmatrix}
r_{11} & r_{12} & \dots & r_{1k} \\
 &\cdots & \cdots & \\
r_{n1} & r_{n2} & \dots & r_{nk}
\end{pmatrix}, 
\quad
r_{ij} :=\text{rank}\left(a_{ij}~ \text{inside}~\text{col}_j(A)\right).
\end{equation*}
Before fitting a parametric model to $\text{row}_i(R)$, we check whether $t_i$ is non-informative. That is,
\[
r_{ij} \sim \text{Unif}(\{u_i, \dots, w_i\}), \quad 1 \le j \le k,
\]
where $u_i$ and $w_i$ are unknown parameters such that $1\le u_i \le w_i \le n$. One can either use a statistical test or apply an approach introduced in paragraph \textit{Information Criterion}. 

From now on, we set $t_i$ to be a selective node. For simplicity, we omit the index $i$ and write $
\text{row}_{i}(R) = (r_{1}, \dots, r_{k})$. We assume that each $r_j$ comes from a mixture of uniform distributions on the discrete grid $\{u, \dots, w\}$, where $1\le u < w \le n$,
\begin{equation}
\label{def:mixture_model}
r_j \sim p^* \text{Unif}\left(\{u, \dots, m^*\}\right) + (1-p^*)\text{Unif}\left(\{m^{*}+1, \dots, w\}\right),
\end{equation}
where $p^* \in (0, 1)$, as well as $u$, $m^*$ and $w$ are unknown parameters. The corresponding ``friends'' set is
$F = \{c_j\in C:\, r_{j} \le m^*\}$. Consequently, the probability to observe $r_l \in F$ is $\frac{p^{*}}{m^* - u + 1}$, and the probability that $r_l \in C\setminus F$ is $\frac{1 - p^{*}}{w - m^*}$.
To estimate the parameters from data, we use Maximum Likelihood. Specifically, the log-likelihood of the mixture~\eqref{def:mixture_model} is
\[
L(p, m, u, w; r_1, \dots, r_k) := s\ln\left(\frac{p}{m-u+1}\right) + (k-s)\ln\left(\frac{1-p}{w - m}\right),
\]
where $ s$ is such that $u \le r_{s} \le m$ and $m < r_{s+1} \le w$. We estimate $u$ as $\min_{j} r_j$ and $w$ as $\max_{j} r_j$. Further, optimizing $L(p, m; r_1, \dots, r_k)$ over $p$ yields $\hat{p} = \frac{s}{k}$. This ensures that one can use brute-force search over $m$ to find the optimal $\hat{m}$. The corresponding ``friends'' set is $\hat{F} = \{c_j \in C:\, r_j \le \hat{m} \}$.

\paragraph{Information criterion}
As an alternative to a uniformity test, we introduce an information criterion relying on a preliminary guess about the dataset in hand. Suppose we believe a priori that any given row vertex is selective with probability $q\in (0, 1)$ and set
\[
L_1 := L(\hat{p}, \hat{m}, \hat{u}, \hat{w}) + \ln(q),
\quad
L_2 := L(0, 0, \hat{u}, \hat{w}) + \ln(1-q),
\]
where $L_1$ is the likelihood under the model that vertex $t$ is selective, and $L_2$ is a competing model corresponding to non-informative vertex $t$.
The best fit is $\max\{L_1, L_2\}$.

\section{Application examples}
\label{sec:experiments}
\subsection{\red{Senteiment analysis}}
\paragraph{Data} To illustrate the performance of the 
\textbf{friends} test, we use AffectVec data \red{[cite]}. This is a word emotion database capturing the subtlety of the English language by providing over 70,000 words annotated with intensity scores for more than 200 emotions. AffectVec quantifies the degree to which each word evokes a wide range of emotional responses. For example, the word ``prank'' may primarily convey joy. Yet it can also be associated with fear, suspense, or a blend of other emotions.

AffectVec is organized as a tabular. Each row corresponds to an individual word and each column represents one of the more than 200 emotion categories. In other words, words are \textit{tags}, emotion categories are \textit{nodes} and the corresponding intensity scores are \textit{attentions}. 

\paragraph{Data preprocessing} To test the \textbf{friends} test, we selected $1080$ adjectives (\red{using Python}). \red{Appendix} presents the selection procedure. The data is available at \red{url}.

\red{We consider two settings}: friends, anti-friends. Full list of emotions and manually filtered list of emotions.

Multiplicity correction: $\frac{1}{1080}$

\subsection{Experiment by AF...}
\section{Discussion}
\label{sec:discussion}

The method works even if the modeling assumption is misspecified...

\textcolor{blue}{Sensitivity to outliers!!!}
\textcolor{red}{We note that the step function can be replaced by some other shapes... Why step? Clustering? Bump?}
\textcolor{green}{Works under model misspecification}

In this manuscript, we develop a method and software to detect noteworthy edges in a weighted bipartite graph. We suppose all edges of the graph to be co-directed. The graph models a directed relation (referred to as attention) from the vertices of one part (nodes) to the vertices of another part (tags).

Essentially, the method consists of two steps. First, we use a double-ranking approach to find the putative friends. Then, to validate the friendship hypothesis, we perform a novel statistical test that is distribution-free.

Along with the single node procedure (\test{best friend test}), we suggest its extension for a subset of nodes (\test{friends test}).

The \text{best friend test} is a particular case of the \test{friends test}, but we consider it separately in the software and, hence, in the methods. Namely, \test{friends test} has higher computational costs and it requires multiplicity correction even for one tag (Section \ref{sec:multimurkers}). In many cases, the \text{best friend test} is enough for practical applications. 

Although the problem looks abstract, its solution has numerous straightforward applications. For instance, the detection of the marker genes \cite{stein-obrien_patternmarkers_2017} for expression patterns critically simplifies the biological interpretation of the results of transcription matrix factorization \cite{Stein_2018,Fertig_2016}. Here, the genes are tags and the patterns are nodes. If a pattern is a friend of a gene (see Section \ref{sec:method}), the gene is the marker of the pattern.

However, the theoretical result is limited to the case of an asymmetric attention matrix. If the matrix is symmetric by the design, the null hypothesis does not hold. However, the computational experiment \textcolor{red}{(see Supplement)} shows that the independence proposition \textcolor{red}{can be used}. Thus, the method applies to the analysis of, e.g., distance matrices. 

The first possible area of application is feature selection. By identification of markers, instead of all tags, we can use a relatively small subset for further analysis. Moreover, the identification of friend-marker pairs helps to remove non-specific connections from a graph. 

Second, the proposed method is useful for efficient clustering of a set of selected features. Also, the friendship concept provides a new similarity measure that possibly generates more interpretable clustering, than the clustering with $A$ being a similarity measure.

Another possible direction is knowledge transfer: if we know something new about Augustus, we know something new about his friends. 

% \textcolor{blue}{+ Friedman test }
% %\url{https://en.wikipedia.org/wiki/Friedman_test}


\section{Conflict of interest}
The authors declare no conflict of interest.

\section{Acknowledgements}
AF acknowledges support by National Institutes of Health (NIH) P30CA006973 and 1U01CA253403-01.
Thanks to Daniel Shu and Caedmon Haas for the translation of the motto to gold Latin. 

\bibliography{gene-best-friends}

\bibliographystyle{splncs03}
%\begin{subappendices}

\newcommand{\beginsupplement}{%
 \setcounter{table}{0}
 \renewcommand{\thetable}{S\arabic{table}}%
 \setcounter{figure}{0}
 \renewcommand{\thefigure}{S\arabic{figure}}
 \setcounter{equation}{0}
 \renewcommand{\theequation}{S\arabic{equation}}%
 }

\newpage
\section*{Supplement}
\beginsupplement
\subsection{Data preprocessing} 
The full list of emotion categories used in the experiment is following:
\textit{joy, surprise, trust, anticipation, fear, anger, sadness, disgust, happiness, levity, hate, loyalty, melancholy, anxiety, embarrassment,  regard, stress, gusto, compunction, cynicism, situation, umbrage, favor, meekness, compassion, withdrawal, scare, unrest, calm, courage, despair, fidget, shyness, apathy, hysteria, shadow, resentment, optimism, heartstrings, bonheur, dudgeon, merriment, hope, foreboding, envy, interest, relaxed, cruelty, helplessness,   solicitude, satisfaction,   suspense, fondness, dolor, weakness, electricity, esteem, woe, relieved, wonder, attachment, pessimism, malice, love, compatibility, timidity, blessedness, exultation, tumult, alienation, humility, powerlessness, complacency, gloom, aggression, sensation, antipathy, gloat, doubt, empathy, consciousness, ingratitude, hopelessness, signal, alarm, dislike, stir, distance, smugness, repentance, easiness, friendliness, gravity, displeasure, discouragement, pique, benevolence, chagrin, tension, togetherness, panic, eagerness, pleasure, excitement, mood, animosity, defeatism, worship, repugnance, grudge, euphoria, antagonism, trait, brotherhood, stewing, pity, daze, sympathy, annoyance, encouragement,  buoyancy, devotion, triumph, contempt, belonging, sinking, unhappiness, trepidation, admiration, disapproval, indifference, affection, astonishment, oppression, languor, coolness, liking, behaviour, peace, misogyny, bang, cheerfulness, creeps, agitation, boredom, gratification  hurt, agape, concern, ardor, mourning, harassment, contentment, closeness, surprised, confusion, presage, approval, state, wrath, dander, reverence, content, amusement, indignation, fearlessness, depreciation, expectation, tenderness, misery, depression, forgiveness, willies, fit, comfort, shame, apprehension   delight, jealousy, aggravation, chill, warpath, serene, exuberance, resignation, gratitude, despondency, nirvana, lividity, emotion, disappointment horror, grief, weight, distress, intoxication, irritation, insecurity, pride, fever, rejoicing, impatience, politeness, tranquillity, hilarity, fury, gladness, thing, nausea, calmness, fulfillment, ecstasy, elation, playfulness, exhilaration, titillation, gratefulness, diffidence, radiance, sorrow, confidence, security, ego, hostility, frustration, attrition, angst, shock, preference, enthusiasm, isolation, conscience, scruple, worry, earnestness, malevolence, awe, guilt, identification.}



The filtered list of emotions
\textit{joy, happiness, surprise, gratitude, friendliness, hope, admiration, love, cheerfulness, trust, interest,
compassion, stress, fear, anger, sadness, hate, melancholy, anxiety, despair, powerlessness, aggression, dislike, cynicism, unrest, apathy, hysteria, calm, courage, shyness, 
 optimism, helplessness, weakness, pessimism, humility, antipathy, 
 togetherness, panic, eagerness, pleasure, excitement, euphoria, brotherhood, sympathy, annoyance, triumph, belonging, unhappiness, admiration, disapproval, indifference, affection, astonishment, 
 agitation, boredom, gratification, concern, harassment, contentment, closeness, confusion, approval, amusement, indignation, fearlessness, depreciation, expectation, 
 tenderness, misery, depression, forgiveness, fit, comfort, shame, apprehension, delight, jealousy, aggravation, chill, disappointment, horror, grief, distress, intoxication,
 irritation, insecurity, pride, impatience, politeness, tranquillity, calmness, playfulness, gratefulness, sorrow, confidence, security, ego, frustration, shock, enthusiasm, isolation, worry, guilt.}






\end{document}


